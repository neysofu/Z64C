\chapter{Heuristics training}
\section{TD($\lambda$) \& TD-Leaf($\lambda$)}

* Chess 960 (5%)
* Endgame tables
* Mixed opening book and not during training, so that it can learn to use it
  when useful and not to when it's not.

Critic update algorithm: https://papers.nips.cc/paper/3290-temporal-difference-updating-without-a-learning-rate.pdf

Once I think about the probability distribution, I can use the extra information
to:
* Better drive game search: search more the more indecisive positions, don't
  search in clearly winning or losely positions. This favours quiet positions,
  just like real engines.
* Better train the critic.
* Can it also replace resoluteness factor? Yes. Instead of thinking about the
  most aggressive move, it outputs a simpler action space and I can select with
  the algorithm that I want.

I need to think about the learning process. I have almost everything figured
out.

Temporal difference learning:
Come modificatore uso:
* il numero della mossa
* material imbalance
Vedi
https://pdfs.semanticscholar.org/6e98/bbc9a3dc3ffb5e23abd748bf4e46d1671929.pdf
per migliorare LDleaf.

Procedura di training:
Crea una nuova scacchiera dall'inizio e gioca una partita. Questo per ogni
istanza, ce n'e' una per thread e sono asincrone (opzionale). Non puo' imparare
durante la partita per semplicita'. Durante la partita (giocata con un numero di
nodi fissi per partita) non eliminare le transpositions che non servono piu'.
values con anche i vari parametri e usi TD64 per calcorare gli state values
ottimali. Ci abbini le mosse migliori con una selezione bayesiana delle mosse
(non sempre la migliore) a metti tutto in fila al batch per l'allenamento.
Allenare il policy e' anche semplice (molto curiosamente), ma devo pensare al
TD64. TD64 e' la variante di TDlambda personalizzata per Z64C. TD(64) sara' una
funzione con una type signature particolare e che funziona molto bene.

Come funziona TD(64)?
Ho una lista di previsioni, un risultato della partita e devo trovare dove ci
sono stati errori... Ben cio'! Complicato. Anziche' distribuire equamente la
ricompensa, posso toglierla da una parte. E' plausibile che ci siano degli
errori in mezzo.

Faccio training sulle mosse di KingBase, e allo stesso tempo alleno con TD(64).
Mantengo il learning rate lo stesso e dopo comincio il self-play.

Devo anche usare CTDleaf con lambda 0.7


* Chess 960 (5%)
* Endgame tables
* Mixed opening book and not during training, so that it can learn to use it
  when useful and not to when it's not.


Critic update algorithm: https://papers.nips.cc/paper/3290-temporal-difference-updating-without-a-learning-rate.pdf

Once I think about the probability distribution, I can use the extra information
to:
* Better drive game search: search more the more indecisive positions, don't
  search in clearly winning or losely positions. This favours quiet positions,
  just like real engines.
* Better train the critic.
* Can it also replace resoluteness factor? Yes. Instead of thinking about the
  most aggressive move, it outputs a simpler action space and I can select with
  the algorithm that I want.

I need to think about the learning process. I have almost everything figured
out.

Temporal difference learning:
Come modificatore uso:
* il numero della mossa
* material imbalance
Vedi
https://pdfs.semanticscholar.org/6e98/bbc9a3dc3ffb5e23abd748bf4e46d1671929.pdf
per migliorare LDleaf.

Procedura di training:
Crea una nuova scacchiera dall'inizio e gioca una partita. Questo per ogni
istanza, ce n'e' una per thread e sono asincrone (opzionale). Non puo' imparare
durante la partita per semplicita'. Durante la partita (giocata con un numero di
nodi fissi per partita) non eliminare le transpositions che non servono piu'.
values con anche i vari parametri e usi TD64 per calcorare gli state values
ottimali. Ci abbini le mosse migliori con una selezione bayesiana delle mosse
(non sempre la migliore) a metti tutto in fila al batch per l'allenamento.
Allenare il policy e' anche semplice (molto curiosamente), ma devo pensare al
TD64. TD64 e' la variante di TDlambda personalizzata per Z64C. TD(64) sara' una
funzione con una type signature particolare e che funziona molto bene.

Come funziona TD(64)?
Ho una lista di previsioni, un risultato della partita e devo trovare dove ci
sono stati errori... Ben cio'! Complicato. Anziche' distribuire equamente la
ricompensa, posso toglierla da una parte. E' plausibile che ci siano degli
errori in mezzo.

Faccio training sulle mosse di KingBase, e allo stesso tempo alleno con TD(64).
Mantengo il learning rate lo stesso e dopo comincio il self-play.

Devo anche usare CTDleaf con lambda 0.7


* Chess 960 (5%)
* Endgame tables
* Mixed opening book and not during training, so that it can learn to use it
  when useful and not to when it's not.

\section{Optimal settings during the training phase}
\section{\ac{MCTS} variations}
\section{An outline of the training process}
