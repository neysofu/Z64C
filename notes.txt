Search algo: CUSTOM WITH NEURAL NETWORK but it's similar to MCTS

Possible AlphaZero missing feature: AlphaZero does not understand when to look
deeper! This is a huge deal and I can take advantage of it. Also, AlphaZero
doesn't know how to incrementally evaluate, thus making search much slower. The
program also does not search during the opponent's turn! BTW: thinking during
the opponent's turn is useful to save time in the next turn, thus one must also
consider the opponent's strength. Evaluation must also be tunable, i.e. it can
evaluate only a subset of the chessboard if necessary, to increase speed.

IT NEEDS TO BE ABLE TO LEARN EVERYTHING IT NEEDS TO KNOW TO PLAY FLAWLESSLY.

Invalid moves are approximated within reasonable limits.

ARCHITECTURE
============

Everything revolves around a chessboard vector. A chessboard vector represents a
thinking instance about a specific chess position. Many chessboard vectors
for the same chess position may be spawn, to better focus about certain features
of the chess position (e.g. central pawns, king safety, material exchange,
ecc.). When presented with a game, Z64C builds a database of chessboard vectors
and analyzes them, looking deeper and deeper into its chess position by using an
attention mechanism, and also by looking into tactics and specific lines. How do
I then reuse the chessboard vectors after I forced a certain move? This is a
major problem. In general, the architecture is much less flexible than
AlphaZero's, but better suited to play real games.

What features does the engine need?

1. Starting a game from arbitrary positions.
   This I can do. I only need to train a neural network from one-hot encoding of
   a chess position to a chessboard vector.
2. Analyze a given position and offer best lines, estimated score, and best
moves.
   I can already give an estimated score of any chessboard vector, but I still
   need to figure out the search algorithm as well as the lines/moves
   calculations. I need to think about other search algorithms, MCTS doesn't
   apply here because I have no information on the inside of the tree's
   information, so I need to have the engine manage it on its own. At the
   moment, Z64C wouldn't even know how to look for the chosen move *after* the
   search... something is broken! The chessboard vector is a cool idea, but I
   need to rethink the whole architecture. The chessboard vectors in the search
   tree need to at least be "indexable" and associated with a chess position, so
   that they can be retrieved when the given chess position is met. At the
   moment, I don't even know what search is guided by!
   Idea:
   Every chessboard vector is associated with one (possibly more) actual chess
   positions. This way the search tree is not completely unstructured and I can
   peep into it.
   Ex. I start with a chess position. I create the main chessboard vector
   associated with that chess position and store it in the Redis instance.
   At this point, I can either keep looking into the chessboard vector or switch
   to another chess position and start looking into that.
   Ideally, I could train the network so that it can handle chessboard vectors
   that apply to many chess positions.

   Instead of a tree, I look at search as operating on a linked list. This
   better fits the reasoning that humans perform: we look at a bunch of chess
   positions, but in an orderly fashion. At the beginning, the chessboard vector
   is just one:

     A

   Its priority is 1.00 and thus will be exploited. After it is exploited,
   another chessboard vector is found: possibly another chess position, or maybe
   the same.

     A
	 B

   As Z64C searchs the action space, the cached chessboard vectors might look
   like this:

     A
	 C
	 B
	 F
	 D
	 E

   Once the timing mechanism blocks further searching, the move associated with
   the topmost chessboard vector is performed.

   How does exploitation work?

   Basically, Redis stores every chessboard vector along with certain
   information:

   - Chessboard vector
   - Estimated score
   - Last half-move

   To exploit, we just need to plug the chessboard vector into the neural
   network, create a new entry for the novel chessbaord vector and update the
   current entry. This way every chessboard vector is uniquely associated with a
   single chess position. Is this a good thing or a bad thing? I really don't
   know... On one hand, it seems like a chessboard vector could possibly be
   linked to many chess positions. But a chessboard vector always (except maybe
   in endgames, where it's not important anyway) contains all necessary
   information to retrieve a specific chess position! This means that it can't
   generalize. Moreover, I want very good instincts and not deep search (unless
   required by tacticts and such).

   Idea:
   how can I look harder at a certain chess position without looking deeper into
   the search tree? If Z64C is able to do this, then it's the ultimate chess
   engine and perfectly simulates human play while being superhuman: more human
   than humans. I need an architecture that allows me to refine and refine a
   chessboard vector, thus obtaining the perfect chessboard vector for a given
   chess position. Sometimes a move changes very slightly the chessboard vector
   and is a "quiet" move, but other times a move is a complete game changer and
   Z64C should be able to look deeper into that. It's not very clear how such an
   architecture would work. It can think as long as it wants for any chessboard
   vector, similarly to a RNN. It's also hard to think where such mechanism
   would lie in the architecture: at move generation, as an external network, or
   what? I think it should be a pluggable, separate network, so that the
   architecture becomes even more flexible (it is currently very flexible and I
   want to keep it that way). So, I need to finally make a decision regarding
   the architecture! I feel like I've made a lot of progress and it's finally
   almost ready. I just need to figure this out, then write learning script.

   Notes about the move generation process: this is the very core of the Z64C
   engine and needs to be absolute perfect. I cannot afford mistakes. A
   chessboard vector is not only a chess position, but also a plan with several
   options and looks for the best options first and the worst later. When a move
   is finally selected, the chosen plan modifies the whole vector and thus the
   process perftectly captures the dynamic nature of the game without problems.
   It's not a hack: the chess position is one thing only with also the plan that
   Z64C is thinking about carrying out and cannot be separated! This can also be
   used to compare chessboard vectors to judge a player's strength: if we are
   given a bunch of chessboard vectors, we can perform clustering and estimate
   which vectors are from the same players. Now I need to think hard about the
   exploitan aspect: how to think harder about a chess position without looking
   deeper into the search tree.

   Features:
   - Chessboard vector [1024]
   - Turn [0/1]
   - Half moves clock [0-1: x/50]
   - Full half moves counter [0-1: x/(x+80)]
   - Castling rights [4]
   - Resoluteness factor [0-1]

   Output:
   - Chessboard vector [1024]
   - Rank of source square [0-1]
   - File of source square [0-1]
   - Rank of target square [0-1]
   - File of target square [0-1]
   - Estimated winning score [0-1]

   Idea:
   what if I imagine thinking process to happen on a long assembly chain that
   goes forward and can branch? I don't think it can work.

   I NEED TO FIGURE OUT THE REFINE NETWORK!!! I am short on time. A few days
   top.

3. Estimate the strength of a given move.
   This is also possible. First the search the chess position before said move
   for some time, then perform the move and search for some more time. At this
   point, the score of the move can be calculated by comparing the estimated
   score of the game before and after moving.
4. Incrementally modify a game while retaining all its calculations (moves in a
normal game.)
   Seems to be possible while retaining chess position information packed with
   the associated chessboard vectors.

Resignation happens when the probability of winning drops below [0.05].

1. Evaluation function.

   Time control:
     There are so many time controls that I wouldn't know which ones to choose,
	 so I represent every time control with a pressure indicator for white [1]
	 and for black [1], which indicates how intricate the position is for that
	 player. The MCTS is then tuned according to parameters specific to the time
	 control. In this way, adding new time controls is straightforward. Along
	 the game tree, a future timeline is mantained. Ex. (10 + 5):

	   600
	   605
	   610
	   615
	   620
	   625
	   630

	 While calculating every move, the engine subtracts the past time to the
	 value.

	   15s hourglass with 5s delay:

	   15

   Problem statement:

   Note:
     only the leaves of the search tree must remember game positions and stuff,
	 all the others can just remember the move to reach that state. The game
	 position is thus carried down the tree and cloned if necessary. This way
	 the board representation must also remember the history by hashing.

   Learning process:
     First, accumulate many, many vectors of chessboards and store them in a
	 database. Millions, if not billions. At that point, from each of those,
	 train the network to generalize the embedding to unseen positions by
	 walking over the graph (i.e. self-play).

   Finally, it is possible to train another network that, given a one-not
   encoding of the chess position, transforms it into a vector representation.
   This is necessary if we want to just load and analyze positions (which is a
   desiderable feature). However, this only happens after the main evaluation
   network is trained and without any changes to the original network.

   Input:
	* Chessboard embedding
	* Halfmove clock [0-1] (n. moves divided by 50)
	* Audacity score [0-1] (how much to go for a win instead of a draw)

   Output:
    * Pressure for white [1]
	* Pressure for black [1]
	* Evaluation score [1]

2. MCTS-like search function.

   Input:
      A sequence of moves, each with an associated score [0-1].
   Mechanism:
      For every move suggested by the output of the evaluation function with a
	  certain probability [0-1], the search function randomly evaluates the
	  resulting positions from those moves if a random variable is greater than
	  its probability. The output of a search is also stored to disk according
	  to that probability, so that it may be more easily retrieved at later
	  stages.
   Output:
      A single move.

Main procedure output:
----------------------
Move: the move to perform
Bool: A draw is desiderable (right now. The chess librabry checks the rules and,
if a drawn can be offered or claimed, it does it.)

TRAINING STRATEGY
=================
Training will use a combination of several learning methods, some supervised and
others not. I have at my disposal:

* Chess 960 (5%)
* Endgame tables
* Mixed opening book and not during training, so that it can learn to use it
  when useful and not to when it's not.
