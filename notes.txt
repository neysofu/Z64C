Search algo: CUSTOM WITH NEURAL NETWORK but it's similar to MCTS

Possible AlphaZero missing feature: AlphaZero does not understand when to look
deeper! This is a huge deal and I can take advantage of it. Also, AlphaZero
doesn't know how to incrementally evaluate, thus making search much slower. The
program also does not search during the opponent's turn! BTW: thinking during
the opponent's turn is useful to save time in the next turn, thus one must also
consider the opponent's strength. Evaluation must also be tunable, i.e. it can
evaluate only a subset of the chessboard if necessary, to increase speed.

IT NEEDS TO BE ABLE TO LEARN EVERYTHING IT NEEDS TO KNOW TO PLAY FLAWLESSLY.

Invalid moves are approximated within reasonable limits.

ARCHITECTURE
============

Everything revolves around a chessboard vector. A chessboard vector represents a
thinking instance about a specific chess position. Many chessboard vectors
for the same chess position may be spawn, to better focus about certain features
of the chess position (e.g. central pawns, king safety, material exchange,
ecc.). When presented with a game, Z64C builds a database of chessboard vectors
and analyzes them, looking deeper and deeper into its chess position by using an
attention mechanism, and also by looking into tactics and specific lines. How do
I then reuse the chessboard vectors after I forced a certain move? This is a
major problem. In general, the architecture is much less flexible than
AlphaZero's, but better suited to play real games.

What features does the engine need?

1. Starting a game from arbitrary positions.
   This I can do. I only need to train a neural network from one-hot encoding of
   a chess position to a chessboard vector.
2. Analyze a given position and offer best lines, estimated score, and best
moves.
   I can already give an estimated score of any chessboard vector, but I still
   need to figure out the search algorithm as well as the lines/moves
   calculations. I need to think about other search algorithms, MCTS doesn't
   apply here because I have no information on the inside of the tree's
   information, so I need to have the engine manage it on its own. At the
   moment, Z64C wouldn't even know how to look for the chosen move *after* the
   search... something is broken! The chessboard vector is a cool idea, but I
   need to rethink the whole architecture. The chessboard vectors in the search
   tree need to at least be "indexable" and associated with a chess position, so
   that they can be retrieved when the given chess position is met. At the
   moment, I don't even know what search is guided by!
   Idea:
   Every chessboard vector is associated with one (possibly more) actual chess
   positions. This way the search tree is not completely unstructured and I can
   peep into it.
   Ex. I start with a chess position. I create the main chessboard vector
   associated with that chess position and store it in the Redis instance.
   At this point, I can either keep looking into the chessboard vector or switch
   to another chess position and start looking into that.
   Ideally, I could train the network so that it can handle chessboard vectors
   that apply to many chess positions.

   Instead of a tree, I look at search as operating on a linked list. This
   better fits the reasoning that humans perform: we look at a bunch of chess
   positions, but in an orderly fashion. At the beginning, the chessboard vector
   is just one:

     A

   Its priority is 1.00 and thus will be exploited. After it is exploited,
   another chessboard vector is found: possibly another chess position, or maybe
   the same.

     A
	 B

   As Z64C searchs the action space, the cached chessboard vectors might look
   like this:

     A
	 C
	 B
	 F
	 D
	 E

   Once the timing mechanism blocks further searching, the move associated with
   the topmost chessboard vector is performed.

   How does exploitation work?

   Basically, Redis stores every chessboard vector along with certain
   information:

   - Chessboard vector
   - Estimated score
   - Last half-move

   To exploit, we just need to plug the chessboard vector into the neural
   network, create a new entry for the novel chessbaord vector and update the
   current entry. This way every chessboard vector is uniquely associated with a
   single chess position. Is this a good thing or a bad thing? I really don't
   know... On one hand, it seems like a chessboard vector could possibly be
   linked to many chess positions. But a chessboard vector always (except maybe
   in endgames, where it's not important anyway) contains all necessary
   information to retrieve a specific chess position! This means that it can't
   generalize. Moreover, I want very good instincts and not deep search (unless
   required by tacticts and such).

   Idea:
   how can I look harder at a certain chess position without looking deeper into
   the search tree? If Z64C is able to do this, then it's the ultimate chess
   engine and perfectly simulates human play while being superhuman: more human
   than humans. I need an architecture that allows me to refine and refine a
   chessboard vector, thus obtaining the perfect chessboard vector for a given
   chess position. Sometimes a move changes very slightly the chessboard vector
   and is a "quiet" move, but other times a move is a complete game changer and
   Z64C should be able to look deeper into that. It's not very clear how such an
   architecture would work. It can think as long as it wants for any chessboard
   vector, similarly to a RNN. It's also hard to think where such mechanism
   would lie in the architecture: at move generation, as an external network, or
   what? I think it should be a pluggable, separate network, so that the
   architecture becomes even more flexible (it is currently very flexible and I
   want to keep it that way). So, I need to finally make a decision regarding
   the architecture! I feel like I've made a lot of progress and it's finally
   almost ready. I just need to figure this out, then write learning script.

   Notes about the move generation process: this is the very core of the Z64C
   engine and needs to be absolute perfect. I cannot afford mistakes. A
   chessboard vector is not only a chess position, but also a plan with several
   options and looks for the best options first and the worst later. When a move
   is finally selected, the chosen plan modifies the whole vector and thus the
   process perftectly captures the dynamic nature of the game without problems.
   It's not a hack: the chess position is one thing only with also the plan that
   Z64C is thinking about carrying out and cannot be separated! This can also be
   used to compare chessboard vectors to judge a player's strength: if we are
   given a bunch of chessboard vectors, we can perform clustering and estimate
   which vectors are from the same players. Now I need to think hard about the
   exploitan aspect: how to think harder about a chess position without looking
   deeper into the search tree.

   Features:
   - Chessboard vector [1024]
   - Turn [0/1]
   - Half moves clock [0-1: x/50]
   - Full half moves counter [0-1: x/(x+80)]
   - Castling rights [4]
   - Resoluteness factor [0-1]

   Output:
   - Chessboard vector [1024]
   - Rank of source square [0-1]
   - File of source square [0-1]
   - Rank of target square [0-1]
   - File of target square [0-1]
   - Estimated winning score [0-1]

   Idea:
   what if I imagine thinking process to happen on a long assembly chain that
   goes forward and can branch? I don't think it can work.

   I NEED TO FIGURE OUT THE REFINE NETWORK!!! I am short on time. A few days
   top.

3. Estimate the strength of a given move.
   This is also possible. First the search the chess position before said move
   for some time, then perform the move and search for some more time. At this
   point, the score of the move can be calculated by comparing the estimated
   score of the game before and after moving.
4. Incrementally modify a game while retaining all its calculations (moves in a
normal game.)
   Seems to be possible while retaining chess position information packed with
   the associated chessboard vectors.

Resignation happens when the probability of winning drops below [0.05].

1. Evaluation function.

   Time control:
     There are so many time controls that I wouldn't know which ones to choose,
	 so I represent every time control with a pressure indicator for white [1]
	 and for black [1], which indicates how intricate the position is for that
	 player. The MCTS is then tuned according to parameters specific to the time
	 control. In this way, adding new time controls is straightforward. Along
	 the game tree, a future timeline is mantained. Ex. (10 + 5):

	   600
	   605
	   610
	   615
	   620
	   625
	   630

	 While calculating every move, the engine subtracts the past time to the
	 value.

	   15s hourglass with 5s delay:

	   15

   Problem statement:

   Note:
     only the leaves of the search tree must remember game positions and stuff,
	 all the others can just remember the move to reach that state. The game
	 position is thus carried down the tree and cloned if necessary. This way
	 the board representation must also remember the history by hashing.

   Learning process:
     First, accumulate many, many vectors of chessboards and store them in a
	 database. Millions, if not billions. At that point, from each of those,
	 train the network to generalize the embedding to unseen positions by
	 walking over the graph (i.e. self-play).

   Finally, it is possible to train another network that, given a one-not
   encoding of the chess position, transforms it into a vector representation.
   This is necessary if we want to just load and analyze positions (which is a
   desiderable feature). However, this only happens after the main evaluation
   network is trained and without any changes to the original network.

   Input:
	* Chessboard embedding
	* Halfmove clock [0-1] (n. moves divided by 50)
	* Audacity score [0-1] (how much to go for a win instead of a draw)

   Output:
    * Pressure for white [1]
	* Pressure for black [1]
	* Evaluation score [1]

	IDEA!
	What if I design a new architecture where the recurrent nature of the
	refiner is extended to move generation? I need to figure out how, but it
	couold work. My point is that chess is something where feedforward doesn't
	really work because it a pondering activity: you need to ponder on the board
	to find good combinations. But it may become very hard to train, even though
	I can easily see Z64C starting by pushing central pawns and then starting to
	move better slowly and slowly, finally becoming better. SO: The refiner
	won't be a standalone module if I do this, which is a shame, but ok. At the
	end maybe I will find some other answer to this problem, but for now it
	looks good. How do I convince the net not to spend time where unnecessary? I
	can train it so that the given move and chessboard vector has EXACTLY x
	probability to win the game and based on some metric is gives as output, I
	can decide to make look into it more and more (only at first nodes).

	Note: the chessboard vector is a stack of [64] numbers? Could work.

2. MCTS-like search function.

   Input:
      A sequence of moves, each with an associated score [0-1].
   Mechanism:
      For every move suggested by the output of the evaluation function with a
	  certain probability [0-1], the search function randomly evaluates the
	  resulting positions from those moves if a random variable is greater than
	  its probability. The output of a search is also stored to disk according
	  to that probability, so that it may be more easily retrieved at later
	  stages.
   Output:
      A single move.

Main procedure output:
----------------------
Move: the move to perform
Bool: A draw is desiderable (right now. The chess librabry checks the rules and,
if a drawn can be offered or claimed, it does it.)

TRAINING STRATEGY
=================
Training will use the modular architecture of the engine and train the various
neural networks with the outputs of the others.

I need to train:
* Hot2board
* Board2moves
* Increment2board

I start by creating a population of Board2moves, each with a starting score
representing its relative strength. This score is not limited in the 0-1 range.
For each training iteration, every neural network is trained. The learning rate
mustn't be adjusted for quite some time to let all the networks learn the
basics. I also instantiate one model of Hot2board and Increment2board and
creating one board tensor from a random chess position. The resulting vector is
cloned and each copy is given to one individual from the Board2moves population,
which will generate a certain move. If the architecture generates many possible
moves, just choose one at random and go with that. Push the move to the board
and increment the other player's board vector using the Increment2board model.

So:
* Create a population for each neural network that I want to train and try to
  plug in different individuals in the process, and see which one gives better
  results. During the "challenge" between two architectures, each one of the two
  receives a certain score. The one with the lower score is trained and the
  other is kept the same. This approach is both flexible and efficient, but I
  should find an optimal algorithm to give even small rewards.

Idea:
No population, otherwise it can't be proven that it converges. I need a simpler
RL algorithm that is also more efficient, not multi-agent. There are several
questions that arise:
* How do I reinforce learning without many agents? What approach would a human
  have if they were given a similar problem? They can't spawn more versions of
  themselves, so how do they do it? They would start by exploring the action
  space and learning what actions lead to better rewards, see if there are
  patterns! At the very beginning you would try random moves though, just to
  explore the space. And it would be a smart idea to keep doing some random
  stuff here and there just not to fall into local minima. I would train one
  neural network at the time, and learning would be very fast this way. So, I
  have this net, how do I judge what moves it does? And how do I teach it? Once
  I have the algorithm, I can implement something like Tic Tac Toe and see if it
  works. The algorithm should be optimal. So, what algorithm should I use?!
* Temporal difference! This allows for extremely fast converging times, without
  having to play sooo many games.

SO: I have a network from the board hot encoding to moves and winning score, how
do I train this without dueling? I mustn't think in terms of rewards, but only
in terms of training.

Action selection strategy: this also depends on what the architecture of the net
is and what candidate moves I have. For a single move output without search,
e-greedy (mixed random and best) is the best approach. With this approach, I can
guarantee that the net will start and keep exploring for better moves.

What's the relation between the estimated score [0-1] and the move strenght
[0-1]? Given one, one can find the other. So the move strenght is not relative
to other moves, but only to the estimated score.

I need an optimal algorithm.
Basically, it start playing moves given board positions. I add an exploration
component by playing also random or unlikely moves. At every move, I get an
estimated score and a move.

I'll use A2C and later possibly a modified version (ACKTR and custom solutions,
specifically tailored for chess).

So my net is at the same time an actor and a critic: gives me an action (or more
than one), and an estimated winning score. How do I train this? I want to
bootstrap the winning score. I will train based on the advantage instead, so
that it's smarter about it.

Ok, maybe I got it. Using a normal optimizer like Adam, I backtrace part of the
net to maximize the estimated score, and I train the estimated score based on
the temporal difference learning formula. Now I should be set, I have the
training figured out, and I can devote my time to implement (and train!) Z64C
v0.2.

Question: how do I represent the estimated score? I don't want a single value,
instead I want a fully blown probability distribution. The distribution goes
from 0 to 1, where 0 and 1 are the fixed scores when the game is over. Anything
in between means it is not certain. How many values and what distribution to
choose? For example, I may want to draw a distribution where my enemy can't win,
I'm very close to winning but I may stalemate. How do I show that? I neet to
show uncertainty in an asymmetric way. I think I can describe the distribution
using 2 parameters:
* My score
* Uncertanty
The uncertainty drives the distribution towards the centre because there is
uncertainty.

The distribution is an S-shaped distribution in a [0-1] box both vertically and
horizontally.

Critic update algorithm: https://papers.nips.cc/paper/3290-temporal-difference-updating-without-a-learning-rate.pdf

Once I think about the probability distribution, I can use the extra information
to:
* Better drive game search: search more the more indecisive positions, don't
  search in clearly winning or losely positions. This favours quiet positions,
  just like real engines.
* Better train the critic.
* Can it also replace resoluteness factor? Yes. Instead of thinking about the
  most aggressive move, it outputs a simpler action space and I can select with
  the algorithm that I want.


* Chess 960 (5%)
* Endgame tables
* Mixed opening book and not during training, so that it can learn to use it
  when useful and not to when it's not.
