ARCHITECTURE
============

Z64C is a novel chess engine that features:
* a competitive evaluation function powered by GPU computing,
* UCI support,
* custom playing style by tweaking parameters,
* better heuristics and uncertainty-driven search with feature engineering,
* cluster search (with Redis?),
* division of work between GPU and CPU with asynchronous algorithms, and
* heuristics-driven time management.

All of this is only possible thanks to particular techniques:
* CPU legality checks while the GPU crunches numbers,
* high move selectivity,
* magic bitboards,
* ArrayFire recurrent neural networks,
* feature engineering a subset of chess with a fallback to Arasan for "weird"
  chess positions (very, very small subset).

ON GPU USAGE
============

Z64C is designed from the ground up to be extremly portable and this is seen in
several design choice:
* possibility of opting out of GPU libraries with fallback to CPU,
* small, quantized neural network,
* low memory impact and tools to limit excessive tree growth, and
* written in C99 with minimal dependencies.

ON BOARD REPRESENTATION
=======================

ON THE SEARCH TABLE
===================

How to sample from a discete probability distribution:

For example, I have a biased die with:

1. 0.2
2. 0.8

1. has 0.2 left and 0.8 right.
2. has 0.2 right and 0.8 left.

I basically want to converge to minimax as fast as possible and using little
memory. So the best mechanism is MCTS, but I can't just pretend like that is the
best way to do it. Actually select and expand, select and expand is extremely
slowly from an architecture point of view and it would destroy the cache.
Instead, I develop a set of boosters that have a reusable architecture and I can
decide what to use according to hardware. Some are to be used for CPUS, others
when there's little ram, excetera.

Z64C uses MCTS to look deeper and deeper into lines and tactics, but uses its
own version of the original algorithm which tries to maximise cache usage and
streams data efficiently to GPU. As we can't remove GPU/CPU communication
overhead, we can only resort to smarter algorithms, such that either of the two
always has something to do and never has to wait for the other. Designing a good
memory layout to the search table is the first step in this direction.

Z64C search trees are a collection of memory chuncks, all of the same size. This
size is page aligned for maximum performance and by default is 4kB. Each of
these chuncks stores a fixed number of search nodes with the following
information:

* Children's weights, so that sampling is easy and local to one node only.
* Chess position using bitboards.
* Stats about predictions for that particular node.

The key to making Z64C lies actually more on search than evaluation, because
search takes advantage of many heuristcs that other engines don't have. So I
need an algorithm that, by staying in the CPU and with no cache misses, it has a
good prediction of the best nodes to search next.

When pruning the search table after making a move, Z64C should take on the
opportunity to shuffle things around and improve data locality. Also, contrary
to what MCTS does, it doesn't need to start samling directly on top.

I would like to experiment an open-addressing storing method for moves, such
that the space used to store moves is very small. Advantages:

* Reduced storage

ON THE SEARCH ROUTINE
=====================

Z64C starts search from the top of its search table by creating a visitor. The
visitor architecture is extremely easy to parallelize: one can just add more
visitors and update the search table in more places at the same time. The
visitor works by hammering down the search tree, finding a good minimum while
mantaining exploration. The behaviour of visitors is customizable directly from
C using function pointers. This allows for different playing styles or - more
simply - to explore unsure nodes during training. Exploration routine:

1. Initialization. Create a visitor at the top node.
2. Fall. Descend the tree by choosing a semirandom path.
3. Impact. Evaluate the node at the visitor and add only its children that are
   considered useful.
4. Pruning. Sometimes Step Impact make us understand that a whole branch of the
   tree is not viable.
4. Bouncing. Recursively move the visitor up the tree for a certain number of
   nodes, which is determined by a random value. Go back to stage 2.

Just like MCTS, this algorithm is easily parallelizable, but this one reduces
the amount of nodes that need to be visited when the search table grows bigger
and bigger, because it takes advantage of proximity of nodes.

The search table also needs other features:

* Pruning. I don't want the search table to just get bigger and bigger. It also
  must let some branches go when we realize they're not as promising as we
  thought they would be.
* Node prediction. While the GPU is computing, the CPU is choosing the next
  nodes to feed to the GPU and saving them in a buffer, ready to be streamed in
  a batch.
* Data locality and memory pools. Nodes close in the tree shuld also be close in
  memory, for better data locality and better chance for cache hits.
* Transpositions. Transpositions are stored by Robin Hood hashing with
  a counter of how many times they were met during search. During pruning I know
  when transpositions are actually finished and I can kill them.
  to delete all transpositions
  and then
* Number of children to add at new nodes. Only new children that have a certain
  probability to improve the current score are kept, those that fall under that
  are directly removed.

The problem with TTable is that it's on the RAM, so it's slow. I want Z64C to be
using CPU 100% and this only happens when I have better data locality.
long-term all tactics and lines explored, which can't possibly fit into a
smaller data structure. I also need to figure out a cache-friendly version of
TTable though, such that I can fetch subsets of TTable very quickly.

ON THE NETWORK
==============

Z64C's neural networks are extremely optimized for efficiency and allow very
fast execution even on CPUs. The GPU can handle even more searching, by using a
special algorithm that ships with every Z64C that allows for local searching
starting for a set of positions and never visiting the transition table.
Z64C uses several networks:

* "BOARD_TO_POSITION":
  Convert a bitboard instance to a chess position thinking instance.
  - 1st layer:
    Has all bitboards as an input, and as an output spits heuristic bitboards. I
	want an architecture that can take work with heuristic bitboards as inputs.
	The whole idea is that some sets in the next layer will be set to either one
	or zero depending on other bits around the board.
* "POSITION_BEST_MOVES":
  Finds the most promising moves from a chess position. This is the main network
  and the most optimized. During training, we apply a dropout of randu*randu.
  connected to the others, with weights and activations.
* "POSITION_INCREMENTER":
  Update a position according to a move. Very fast and usually used just before
  POSITION_BEST_MOVES to simulate a RNN. Has the same exact topology of
  POSITION_BEST_MOVES.

All these neural networks have an exotic, novel architecture that uses 1 bit
quantization. Example by showing a 3x3 board with width 1.

110 100 001

At this point I have several inputs:

* Board representation
* Turn [2 booleans]
* Resoluteness [1 float]

Our goal is to approximate a certain function f. f is from inputs to outputs and
we have a specific neural architecture to allow best performance with this
function approximator. The training algorithm can take advantage from many
actors at the same time. Basically, during play, I create many many training
examples. Every descent down the tree improves prediction without exception,
even considering uncertainty. If a line is uncertain, then also its first move
should be uncertain. So I want to explore the tree to approximate the function
in its weak spots, this way it will converge *etremely* fast. I want to study
the approximation strategy because it is important.

Thanks to the approximation strategy and tree search I will have an incoming
stream of labeled examples.

We first train the last layer, which is the only different one. We must train it
from a certain neuron and

instaed of fighting against it.

A chess position in Z64C is represented as a certain number of 64bit numbers,
much like bitboards. I can experiment with any number that I want, 64 seems like
a good number. 64*64=4097 bits per chess position, which is extremely low. A
transformation on this chess position should be studied accordingly to the
chessboard specifics. It presents 1 bit weights and no activation function, so
that a single transformation can take up very little CPU cycles.

EXAMPLES
========

uci
	Z64C starts and is ready to play. The position is the initial one.
position startpos e2e4
	Z64C loads a certain position into memory. Every time a new position is
	loaded into the engine, Z64C resets the search tree.
go ...
	Z64C starts the search.
stop
move e7e6
	Applies the given move to the in-memory board and stores the move in the
	move tree.
bestmove
	d2d4

Search routine
--------------
The search routine is fundamental to a good chess engine and must be as fast as
possible, because it will analyze millions of nodes and time must be saved for
the evaluation function.
Z64C's search routine uses arrays to store the big number of nodes. The whole
thing (just as the model) uses ArrayFire arrays to store everything in-memory.
This allows extremely fast code. It's basically blazing, blazing fast. I need to
find a way to store the search tree as a compact array. Once I've done that, I
associate each node with a score and a move. I randomly go down the tree, expand
it with a new node, and backpropagate up. This many, many times.

I have settled on the fact that I need a custom solution for the search. I
really do, but I can add sherding and read slaves as necessary! It won't be too
hard.

From the chessboard vector, how do I get a legal moves vector? And then how to
modify it. At that point it may become the fundamental chessboard implementation
in the engine, converted then to a inefficient board implementation. An
efficient and inefficient data structure, one to do the heavy lifting and the
other for the simple things. It could also be a perfect representation of the
chessboard vector. It must respect some conditions:

The strategic component of the chessboard must be highligthed. This doesn't mean
that after a single move the vector is almost untouched. This means that the
best moves are closer that the worse moves!
Basically, I want the distance between two chessboard vectors to be equal to the
strength of the move (or sequence of) between the two.

The strength of a move is a metric between 1 and 0, where 0 is a certainly
"worse-ing" move and 1 is a certainly "better-ing" move. The score of a game is
determined by my chance of winning, the opponent's chance of winning and the
uncertainty over these two values.

Una volta che trovo una rappresentazione embedded delle scacchiere con una parte
messa in risalto alla parte strategica, posso allenare una rete shallow per
ottenere una lista delle mosse migliori, con le mosse nonlegali a basso
punteggio o negative. Posso anche allenare una rete neurale per incrementare e
robe varie. La cosa importante e' che ho reinventato la visione degli scacchi.

The embedding must respect some conditions:
* The distance between two embeddings must be greater for similar chessboards,
  especially those easily reachable by a sequence of a grandmaster.
  Basically, the distance between two vectors equals (1 - strength) for each move
  needed to go from here to there.

Allenamento: ho come dataset un albero estremamento grande e profondo.

Devo trasformarlo in un albero di embedding e nel frattempo allenare delle reti
neurali.

A chess tensor is a tensor representing a chess position that satisfies certain
conditions:
* The distance between two chess tensors equals the likelihood that a perfect
  player will play that sequence of moves.

This means that there are infinitely many chess tensors even for a single chess
position, but some are more "true" than others. The dimensionality of chess
tensors is 1024. The distance is cosine similarity.
The distance to illegal positions.

I need to reinvent Z64C!
Basically, the classic search architecture works, but is limited by the huge
branching factor of chess and especially Go. So I need a search procedure that
actually searches inside a specific node and searches the perfect evaluation!
So:

I have a 1024-wide position tensor that is the evaluation of the current
position. This tensor is, however, not optimal. I need to seach the
1024-dimensions search space for ideas and tactics without actually making a
distinction between this or that position! Searching near the original point
basically simulates the search space, but instead of looking deeper and deeper
to improve an evaluation I can just look closer and closer and occasionally
deeper.

Un'altra cosa: a ogni chess tensor non associo una singola scacchiera, bensi'
piu' o meno una scacchiera. Ce ne sara' una piu' vicina alle altre, ma non e'
assolutamente ben distinto.


When Z64C needs to deepen a certain node, it creates and inserts a new node with
a score determined by its search algorithm, and then lowers the score of the
current node to the second best move (or 0 if all moves have been explored).
This assumes that the neural network gives 0 as output for illegal moves.

The questions that arise in this approach are:
* When do you stop searching?
  When I find one (or possibly more) line that has a satisfying score with good
  certainty. This part of the algorithm must take into serious consideration the
  uncertainty of the score, as it is key in the search: spend more time thinking
  in positions where your guts tell you that you might have a chance at getting
  a lead over your opponent. Search until the best lines either tell that you
  where wrong, or you were right. Act accordingly.
* How do you determine a newly created node's score?
  The score is a double-precision float and should be in the range 0-1, so that
  successive plies go to 0. It makes sense! A 0.95 move is almost forced and
  basically all lines follow that single move. The second best move could be
  like 0.03, at that point 0.03 will only be computed before 0.95 has a line
  that brings it down to 0.03. The alternation of colors doesn't alter the
  scores.
* How do you, after all of this, choose the best move?
  I need to formally prove this mathematically, but I have an intuition that can
  be used in a probablistic tree traversal algorithm.
* How do you remove all transpositions unreachable after the enemy's move in a
  short time?
  This also relates to the tree-like indexing of transpositions in the Redis
  server.
  I need to remove all positions (probably) unreacheable from the current position, so:
  This problem arises from the fact that I'm treating a tree (the search tree)
  with a hash table! I need to find an encoding.

Another question is: what do I set on nodes as an expiry date? White's turn
estimate plus black's turn estimate seems like a good choice. I'm gonna do it
like that.

In Redis, the sorted set has the numeric ID as keys.

The hash table has these keys:

* ID:tensor_in
* ID:tensor_out
* ID:board
* ID:parent
* ID:main
* ID:branch

TRAINING STRATEGY
=================
Training will use the modular architecture of the engine and train the various
neural networks with the outputs of the others.

I need to train:
* Hot2board
* Board2moves
* Increment2board

What's the relation between the estimated score [0-1] and the move strenght
[0-1]? Given one, one can find the other. So the move strenght is not relative
to other moves, but only to the estimated score.

I need an optimal algorithm.
Basically, it start playing moves given board positions. I add an exploration
component by playing also random or unlikely moves. At every move, I get an
estimated score and a move.

So my net is at the same time an actor and a critic: gives me an action (or more
than one), and an estimated winning score. How do I train this? I want to
bootstrap the winning score. I will train based on the advantage instead, so
that it's smarter about it.

Ok, maybe I got it. Using a normal optimizer like Adam, I backtrace part of the
net to maximize the estimated score, and I train the estimated score based on
the temporal difference learning formula. Now I should be set, I have the
training figured out, and I can devote my time to implement (and train!) Z64C
v0.2.

Notes on perft:
I want to develop a perft so that I don't need to worry about checks. It would
be much, much faster! What's the quirk?
A move is illegal if, at the next move, there is a pseudolegal move to capture the
king.

I need a struct Board where I can extremely rapidly say which moves are in the
tensor and if they are legal.

Cavalli: 8
Pedoni: 28
Torri: 14
Alfieri: 13
Re: 8

Architettura REDIS:
Sorted set con Id dei search node e priorita'.

Non capisco se dovrei fare l'update delle statistiche... mi serve davvero?
Spero davvero di no, perche' altrimenti sarei nei guai. Diventa un magnitudo di
complessita' temporale piu' lento. Mi serve che la ricerca sia velocissima. La
ricerca deve fare:

* Legality check (pseudo?)
  Come faccio a gestire una pseudo legality checking? Il problema non e' affatto
  lo scacco matto, ma lo stallo, che invece e' difficile da prevedere.

Ricerca:
Parto dal root node che non e' in Redis ma nello struct Engine e da li' aggiungo
a Redis i nodi di ricerca con la relativa priorita'. Una volta finito il tempo
di ricerca, tutti i nodi dell'albero vengono backtraced e si scopre il nodo
migliore.

La struttura in hash ha come key la scacchiera (ottimo), e come valore:
* Position tensor (scalable sia per one-hot che LSTM)
* Mossa della precendente
Il tutto in un memory dump di uno struct compatto by value.

La ricerca fa:
Prendo il nodo al top, prendo il position tensor, ci corro Tensorflow e ottengo
i nuovi nodi che metto dentro Redis.
La priorita' e' data da da una formula che dipende da TRAINING.

Se no training: allora la priorita' e' solo N, altrimenti ci sommo una
distrubuzione bounded normale

Il sorted set rappresenta i branch che devo esplorare con piu' urgenza.

Domanda: come influisce lo stallo sull'allenamento?

Qual e' l'algoritmo di ricerca?

0. Metti in focus il nodo di base.
1. Prendi il nodo di base e scegli una mossa.
non pseudolegali vengono subito eliminate e non inserite.
2. Se

Mi serve anche un algoritmo di backpropagation.

Ho una rete a parte, (7 strati) che mi calcola il passaggio da una scacchiera
ad un'altra. Le scacchiere memorizzano 64 dati per casella, quindi sono 4096 di
spessore.

Inoltre ho anche una rete a parte che mi traduce dal one-hot al tensor.

Quindi ho:
	16 per cavalli (2)
	48 per torri (3)
	48 per alfieri (3)
	32 per pedoni (8)
	 8 per re (1)

    152 move features
	3 state values
	Anche qua, derivo il learning rate da un fattore statistico.

	In input ho la scacchiera o il lstm piu':
	* Turn [2]
	* Castling rights [4]
	* En passant [8]
	Anziche' avere a che fare con i tempi, metto come output una descrizione
	piu' precisa dello state value cosi' so quali sono i punti piu' critici e
	quanto e' sicuro della propria azione.

Ho una architettura GRU con 140 uscite. Creo l'architettura in modo tale che
sia:

* Facilmente allenabile
* Pensi nel tempo e modifichi le propie previsioni (opzionale)
* Riporti la mossa automaticamente al GRU (sara' un GRU particolare, allora).

Question: how do I represent the estimated score? I don't want a single value,
instead I want a fully blown probability distribution. The distribution goes
from 0 to 1, where 0 and 1 are the fixed scores when the game is over. Anything
in between means it is not certain. How many values and what distribution to
choose? For example, I may want to draw a distribution where my enemy can't win,
I'm very close to winning but I may stalemate. How do I show that? I neet to
show uncertainty in an asymmetric way. I think I can describe the distribution
using 2 parameters:
* My score
* Uncertanty
The uncertainty drives the distribution towards the centre because there is
uncertainty.

The distribution is an S-shaped distribution in a [0-1] box both vertically and
horizontally.

Critic update algorithm: https://papers.nips.cc/paper/3290-temporal-difference-updating-without-a-learning-rate.pdf

Once I think about the probability distribution, I can use the extra information
to:
* Better drive game search: search more the more indecisive positions, don't
  search in clearly winning or losely positions. This favours quiet positions,
  just like real engines.
* Better train the critic.
* Can it also replace resoluteness factor? Yes. Instead of thinking about the
  most aggressive move, it outputs a simpler action space and I can select with
  the algorithm that I want.

I need to think about the learning process. I have almost everything figured
out.

Temporal difference learning:
Come modificatore uso:
* il numero della mossa
* material imbalance
Vedi
https://pdfs.semanticscholar.org/6e98/bbc9a3dc3ffb5e23abd748bf4e46d1671929.pdf
per migliorare LDleaf.

Procedura di training:
Crea una nuova scacchiera dall'inizio e gioca una partita. Questo per ogni
istanza, ce n'e' una per thread e sono asincrone (opzionale). Non puo' imparare
durante la partita per semplicita'. Durante la partita (giocata con un numero di
nodi fissi per partita) non eliminare le transpositions che non servono piu'.
values con anche i vari parametri e usi TD64 per calcorare gli state values
ottimali. Ci abbini le mosse migliori con una selezione bayesiana delle mosse
(non sempre la migliore) a metti tutto in fila al batch per l'allenamento.
Allenare il policy e' anche semplice (molto curiosamente), ma devo pensare al
TD64. TD64 e' la variante di TDlambda personalizzata per Z64C. TD(64) sara' una
funzione con una type signature particolare e che funziona molto bene.

Come funziona TD(64)?
Ho una lista di previsioni, un risultato della partita e devo trovare dove ci
sono stati errori... Ben cio'! Complicato. Anziche' distribuire equamente la
ricompensa, posso toglierla da una parte. E' plausibile che ci siano degli
errori in mezzo.

Faccio training sulle mosse di KingBase, e allo stesso tempo alleno con TD(64).
Mantengo il learning rate lo stesso e dopo comincio il self-play.

Devo anche usare CTDleaf con lambda 0.7


* Chess 960 (5%)
* Endgame tables
* Mixed opening book and not during training, so that it can learn to use it
  when useful and not to when it's not.
