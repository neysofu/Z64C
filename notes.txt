ON THE APIS
===========

Commands are just strings with arguments, so...
It would make sense to use MsgPack though, because it is a standard and is fast.

ARCHITECTURE
============

Z64C is a novel chess engine that features:
* a competitive evaluation function powered by GPU computing,
* UCI support,
* custom playing style by tweaking parameters,
* better heuristics and uncertainty-driven search with feature engineering,
* cluster search (with Redis?),
* division of work between GPU and CPU with asynchronous algorithms, and
* heuristics-driven time management.

All of this is only possible thanks to particular techniques:
* CPU legality checks while the GPU crunches numbers,
* high move selectivity,
* magic bitboards,
* ArrayFire recurrent neural networks,
* feature engineering a subset of chess with a fallback to Arasan for "weird"
  chess positions (very, very small subset).

Note for the transpositions table:
Maybe I can use a range based solution with binary tree.
Let's suppose I have 4 elements. Then I would need 2 levels of binary tree. For
2**n levels, I need N levels. What if I can keep that number constant or
logarithmic?

I set a certain k=16 and every level is an array that reduces the number of
elements by even more. Instead of having binary search, I can use fast bitshift
together with cache locality for good behaviour!

I am looking for a solution with:
* Fast search and insert.
* Extremely good scaling.
* Dynamic resizing.

I can dispatch from a table that divides into 256 sectors, each with a pointer
-> 64 cache lines.
This will already divide by a factor of 2**6 the workload.
It must support:
- Probing
- Setting
- (Optional.) Dynamic growth
- Shrinking

API
===
Un chess engine puo giocare a scacchi. Cosa devo esporre con le APIs? Capacita'
di giocare, e giocare vuol dire scegliere una mossa data una certa posizione.
Estendiamo il concetto a intere partite, che sono dall'inizio alla fine.
L'analisi invece si tratta di scoprire linee alternative.

Basically what is missing here is the search. How the hell do people search?
Engines go one level by level, but this is wasteful of resources and suboptimal.

I need a system that can explore many paths without remembering them... How the
hell do I do that?
The answer here is that there is not one. It's about tradeoffs with space and
time and computer architecture. A good solution might be to explore branches
from the down up by estimating how much time it might take to explore this
branch. How do I want the search to go. Othewise, I can store everything... but
it will take everything!

I have 64 bit keys and need a unique address for each of them.

Best search algorithm:
* It must be fast, i.e. search overheard must be small compared to evaluation.
  It must spend time thinking about chess positions, not shuffling bytes.
* It must be selective.
* It must have low memory consumptions.
No things such as 

So the search is:
* Choose a path and deepen it.

So I have a core data structure that indexes one byte of data. 256 buckets (data
& 0xff and then >> 8), each of those buckets points to a new bucket. This can
also be dynamic...

Low memory consumption...
1. I start at the top position and evaluate it. Any evaluation is added to the
transpositions table list.
2. Deepen the most interesting node and go down with it, store the good result
into memory.
3. I can investigate optimal search algorithms, but the core strategy is the
same.

SEARCH
======

I think I have a nice idea, but need to consider something before deciding it's
worth it.

What other engine do is they just look at many many positions, which is silly
because most of them are absolutely moronic. I just want to look at moves that
seem interesting... for this it is necessary to have an excellent evaluation
function, but I trust that I can have one.

A kind of best first approach!

- I have the top position and an empty transposition table.
- I evaluate the top node and push all evaluations on the TTable.
- Go down one ply.
- Deepen all moves that are better than X.

This way I never need to store board tensors and output tensors are immediately
transformed into transpositions.
What's the problem though? The problem is always the position, that I cannot
easily store because it's so freaking huge. Not storing it would however cause a
very expensive recomputation process.
On one hand, use very little memory and recompute stuff a lot.
On the other hand, each node computation takes more time than because it is
written. The question is:
* Is it better to store board tensors and have more accurate predictions or less
  accurate predictions? And also, what is the best search algorithm that is
  compatible with my goals?

* Monte Carlo tree search
  - High memory consumption
  - Cache pollution
  - Slowish search
* Alpha beta pruning
  - Blind evaluation
  - Evaluates way too many nodes, even useless ones
* Monte Carlo Alpha Beta
  - More efficient than pure MCTS
  - Requires move generation for board


ON GPU USAGE
============

The goal of Z64C is to be a wonderful implementation of the most optimal chess
algorithm possibly existing, while being an open an easily tunable algorithm.
This algorithm should fall gracely into the architecture of commodity hardware
as well as specialized one, i.e. doesn't require any special architecture and is
flexible.

The core algorithm works with a concept known as bubbles. Bubbles improve
locality of reference than plain transpositions tables. Z64C can blow bubbles
concurrently, so the algorithm is extremely flexible. One can either use large
bubbles and less threads or few bubbles. Bubbles are groups of positions that
are evaluated together for better locality. Every group is grown independently
and has:
* A belt
* A batch
* One thread or more

The bubble architecture must be very fast and local, both spacial and temporal
locality. This will be the single best algorithm because it is flexible enough
to use it with many different architectures. At the start of the game, there is
a single empty bubble. Blowing bubbles works like this:

* You pick a bubble and decide to blow it. You act now independently from all
  other bubbles.
* Inside this bubble, positions are stored using open addressing hashing with
  techniques such as linear probing or Robin Hood.
* Bubbles and the elements inside are enumerated. Numbers close to the current
  number are intra-bubble positions.
* Bubbles should hold positions that can only become positions in the same
  bubble.

Ideas for bubbles:
* Depth based: every bubble stores a certain number of plies down the line, so
  that detecting extra-bubble outbursts would become extremely easy. Bubbles
  could be very easy to set up, just choose a size and it will fill up.
* Only reversable moves are in the bubble. This is very interesting as well.

I can consider bubbles that are soft limited and that use open addressing
schemes that are relatively local in reference.

The problem is always extra-table references though...
What can I do? I need a custom hashing algorithm, possibly many so that I can
choose the best:

* xxHash with linear probing or any other open addressing scheme.
*

So basically I should store in different parts of the program:
* MCTS (I can even store this to disk or cold memory. It doesn't really matter speed of
  fetching, as I only need to store few elements)
* Actual data
By keeping together the MCTS, I minimize cache line fills for reading node data
and also to incrementally load that.

BUT! What if there's a... O(1) !! solution to my problem. I feel like there
should be. Just like discrete ditribution sampling, every node that becomes a
little more probable should be better than all the others.

Fuck. I just realized that the transitions table can't be dynamically allocated.
What if I used an ordered array instead of hashing?

After a short walk I came up with a great new bubble architecture! I was
constantly thinking about bubbles indexed by hash, instead, I have a very very
flexible (and rather simple) idea for bubbles:

 Use self-balancing ordered search trees.

What I need is an approximate value for each board, but the nice thing is that I
don't need it to be unique.

Ok, I decided that I'll think about it. How can I deal with more moves than
usual?

I can just fit the whole thing into the buffer and then

Basically I got a huge tree (which is effectively a double tree, because it
contains pointers both for )

The base working system is:
many threads, each with their own task. Every thread adds tasks to schudules,
then are merged into the system.

Now I need to consider batch search, ttable, and evaluation, because everything
is linked together.
The question is: do I need to store all info in a node? Maybe not. This is
because, after I looked through all the possible children in a certain node,
what good are its tensor_in, tensor_out, and board? Absolutely nothing! struct
Board may still be useful, but I expect to be able to let go all the tensor_in
and out data. Children data unfortunately still need to be explored.

Now I consider searching. I have a decision tree with a nice evaluation function
that is very slow to kickstart and stop for a certain node, so I need to reduce
that to a minimum.

1. Select. The visitor randomly selects a terminal node. Terminal nodes are the
nodes that need further exploration, because otherwise their children would be
explored as well. Terminal nodes that don't need further exploration are marked
so that they are not visited.
2. Explore. The exploration phase became more prominent after some
considerations about memory requirements and cache. The exploration phase
basically exhausts a given path by keeping on the stack a tensor_in. That
tensor_in is applied to

I don't think I can flatten the whole tree, because I still need to highlight
parent relationships. If I flatten everything, then how can I possibly alter the
frequencies of other children? I can't. BUT the approach that I'm taking has a
very low upper bound, i.e. it can be extremely fast.
If I'm able to do some flattening, what I can achieve is:
* Extremely good cache behaviour. This is a huge win for performance.
* More predictable multithreading, maybe lockless and without slowing them down.

This means that my search algorithm will not be pure Monte Carlo, because pure
Monte Carlo spreads exploration way too thin. Just one prediction mistake and a
whole branch won't be considered. Instead, I will choose nodes and then do a
shallow search.

What's the maximum number of transposition references? A position is only
referenced from source positions, right? So the answer is how many possible
antimoves can I make? Not possibly more than 256, so the reference count will
always be lower than that!

* 0.2 (1/2 * 0.4)
* 0.8 (1/2 * 0.4 + 1/2 * 1)

* 0.1 (1/3 * 0.3)
* 0.2 (1/3 * 0.35 + 1/3 * 0.25)
* 0.7 (1/3 * 0.35 + 1/3 * 0.75 + 1/3 * 1)

They are sorted in ascending order.

1. Choose one element at random. That element will determine my probability
distribution.
2. I get a random number from 0 to 1 according to that distribution and
distibute it to the rightmost elements.

I thus have:
* 1.8 (any)
* 0.5

Algorithm:
 1. Choose at random one element from frequencies with uniform probability.
 2. Let x be p + rand() * (1-p)
 3. Return n. i * (1-p).

Frequencies:
	* 0.05
	* 0.2
	* 0.21
	* 0.54
I want to simulate this in O(1).

So, I fill a bubble with positions until they can fit no more. But what happens
then? I create a new bubble and ideally should fit any other kind of positions
there, but it won't happen.

But wait! Maybe I can get the best out of both worlds (complex data locality
with bubbles architecture and flat and easy structure of TTable) and also
improve speed by using a flat yet customizable architecture: instead of doing
random read and writes, I stay local and use a semirandom algorithm to create a
semirandom walking path. This way, the computation will be bound by:
* Legality checking
* Move pushing
* Thus CPU
* And obviously GPU computation and ANN evaluation.

This architecture is also flexible because if I have some region of memory that
accesses faster than others, I can hack the software into treating it the same
exact way as I usually do and I place explorating agents on the faster memory
portion.

Now, however, I need to define a good semirandom path finder. This semirandom
path finder must be incredibily easy on the CPU and must converge to minimax.


TTable architecture:

Each node must contain:
* Path finder via open addressing
* The current board position
* Input tensor
* Output tensor (?)

Every agent follows a unique desire path. How do I get a desire path? The path
should ONLY originate from the original 64bit seed, nothing more.

The desire path strategy operates on the original tensors? Nah, they are
useless. I only need to story frequencies according to the paper. But the desire
path strategy how does it work? It's basically a deterministic value that,
starting from the seed, oscillates according to some very fast operations and
tells me what to choose. This way I don't have to generate a lot of data.

Z64C is designed from the ground up to run fast on a variety of commodity
hardware. The goal is to have as flat as a profile as possible. The core MCTS
exploration can be made rather simple, doable by very simple threads and thus
benefits for parallelism. The GPU, on the other hand, can:
* Run evaluation functions in batches.
* Do descent.
* Training.
Heavy work that the CPU must compute:
* Chess move
* Legality checking.
I think I can overcome these by using pseudolegal moves, then legality checking
really will be very fast.

I think the goal here is to:
Make everything parallelizable and concurrent and fast. Evaluation and
generation already is, many cores at that point should be able to sustain all of
that. I cannot reasonably expect to do everything on the CPU.

ON BOARD REPRESENTATION
=======================

ON THE SEARCH TABLE
===================

How to sample from a discete probability distribution:

For example, I have a biased die with:

1. 0.2
2. 0.8

1. has 0.2 left and 0.8 right.
2. has 0.2 right and 0.8 left.

I basically want to converge to minimax as fast as possible and using little
memory. So the best mechanism is MCTS, but I can't just pretend like that is the
best way to do it. Actually select and expand, select and expand is extremely
slowly from an architecture point of view and it would destroy the cache.
Instead, I develop a set of boosters that have a reusable architecture and I can
decide what to use according to hardware. Some are to be used for CPUS, others
when there's little ram, excetera.

Z64C uses MCTS to look deeper and deeper into lines and tactics, but uses its
own version of the original algorithm which tries to maximise cache usage and
streams data efficiently to GPU. As we can't remove GPU/CPU communication
overhead, we can only resort to smarter algorithms, such that either of the two
always has something to do and never has to wait for the other. Designing a good
memory layout to the search table is the first step in this direction.

Z64C search trees are a collection of memory chuncks, all of the same size. This
size is page aligned for maximum performance and by default is 4kB. Each of
these chuncks stores a fixed number of search nodes with the following
information:

* Children's weights, so that sampling is easy and local to one node only.
* Chess position using bitboards.
* Stats about predictions for that particular node.

The key to making Z64C lies actually more on search than evaluation, because
search takes advantage of many heuristcs that other engines don't have. So I
need an algorithm that, by staying in the CPU and with no cache misses, it has a
good prediction of the best nodes to search next.

When pruning the search table after making a move, Z64C should take on the
opportunity to shuffle things around and improve data locality. Also, contrary
to what MCTS does, it doesn't need to start samling directly on top.

I would like to experiment an open-addressing storing method for moves, such
that the space used to store moves is very small. Advantages:

* Reduced storage

ON THE SEARCH ROUTINE
=====================

Z64C starts search from the top of its search table by creating a visitor. The
visitor architecture is extremely easy to parallelize: one can just add more
visitors and update the search table in more places at the same time. The
visitor works by hammering down the search tree, finding a good minimum while
mantaining exploration. The behaviour of visitors is customizable directly from
C using function pointers. This allows for different playing styles or - more
simply - to explore unsure nodes during training. Exploration routine:

1. Initialization. Create a visitor at the top node.
2. Fall. Descend the tree by choosing a semirandom path.
3. Impact. Evaluate the node at the visitor and add only its children that are
   considered useful.
4. Pruning. Sometimes Step Impact make us understand that a whole branch of the
   tree is not viable.
4. Bouncing. Recursively move the visitor up the tree for a certain number of
   nodes, which is determined by a random value. Go back to stage 2.

Just like MCTS, this algorithm is easily parallelizable, but this one reduces
the amount of nodes that need to be visited when the search table grows bigger
and bigger, because it takes advantage of proximity of nodes.

The search table also needs other features:

* Pruning. I don't want the search table to just get bigger and bigger. It also
  must let some branches go when we realize they're not as promising as we
  thought they would be.
* Node prediction. While the GPU is computing, the CPU is choosing the next
  nodes to feed to the GPU and saving them in a buffer, ready to be streamed in
  a batch.
* Data locality and memory pools. Nodes close in the tree shuld also be close in
  memory, for better data locality and better chance for cache hits.
* Transpositions. Transpositions are stored by Robin Hood hashing with
  a counter of how many times they were met during search. During pruning I know
  when transpositions are actually finished and I can kill them.
  to delete all transpositions
  and then
* Number of children to add at new nodes. Only new children that have a certain
  probability to improve the current score are kept, those that fall under that
  are directly removed.

The problem with TTable is that it's on the RAM, so it's slow. I want Z64C to be
using CPU 100% and this only happens when I have better data locality.
long-term all tactics and lines explored, which can't possibly fit into a
smaller data structure. I also need to figure out a cache-friendly version of
TTable though, such that I can fetch subsets of TTable very quickly.

ON THE NETWORK
==============

Z64C's neural networks are extremely optimized for efficiency and allow very
fast execution even on CPUs. The GPU can handle even more searching, by using a
special algorithm that ships with every Z64C that allows for local searching
starting for a set of positions and never visiting the transition table.
Z64C uses several networks:

* "BOARD_TO_POSITION":
  Convert a bitboard instance to a chess position thinking instance.
  - 1st layer:
    Has all bitboards as an input, and as an output spits heuristic bitboards. I
	want an architecture that can take work with heuristic bitboards as inputs.
	The whole idea is that some sets in the next layer will be set to either one
	or zero depending on other bits around the board.
* "POSITION_BEST_MOVES":
  Finds the most promising moves from a chess position. This is the main network
  and the most optimized. During training, we apply a dropout of randu*randu.
  connected to the others, with weights and activations.
* "POSITION_INCREMENTER":
  Update a position according to a move. Very fast and usually used just before
  POSITION_BEST_MOVES to simulate a RNN. Has the same exact topology of
  POSITION_BEST_MOVES.

All these neural networks have an exotic, novel architecture that uses 1 bit
quantization. Example by showing a 3x3 board with width 1.

110 100 001

At this point I have several inputs:

* Board representation
* Turn [2 booleans]
* Resoluteness [1 float]

Our goal is to approximate a certain function f. f is from inputs to outputs and
we have a specific neural architecture to allow best performance with this
function approximator. The training algorithm can take advantage from many
actors at the same time. Basically, during play, I create many many training
examples. Every descent down the tree improves prediction without exception,
even considering uncertainty. If a line is uncertain, then also its first move
should be uncertain. So I want to explore the tree to approximate the function
in its weak spots, this way it will converge *etremely* fast. I want to study
the approximation strategy because it is important.

Thanks to the approximation strategy and tree search I will have an incoming
stream of labeled examples.

We first train the last layer, which is the only different one. We must train it
from a certain neuron and

instaed of fighting against it.

A chess position in Z64C is represented as a certain number of 64bit numbers,
much like bitboards. I can experiment with any number that I want, 64 seems like
a good number. 64*64=4097 bits per chess position, which is extremely low. A
transformation on this chess position should be studied accordingly to the
chessboard specifics. It presents 1 bit weights and no activation function, so
that a single transformation can take up very little CPU cycles.

EXAMPLES
========

uci
	Z64C starts and is ready to play. The position is the initial one.
position startpos e2e4
	Z64C loads a certain position into memory. Every time a new position is
	loaded into the engine, Z64C resets the search tree.
go ...
	Z64C starts the search.
stop
move e7e6
	Applies the given move to the in-memory board and stores the move in the
	move tree.
bestmove
	d2d4

Search routine
--------------
The search routine is fundamental to a good chess engine and must be as fast as
possible, because it will analyze millions of nodes and time must be saved for
the evaluation function.
Z64C's search routine uses arrays to store the big number of nodes. The whole
thing (just as the model) uses ArrayFire arrays to store everything in-memory.
This allows extremely fast code. It's basically blazing, blazing fast. I need to
find a way to store the search tree as a compact array. Once I've done that, I
associate each node with a score and a move. I randomly go down the tree, expand
it with a new node, and backpropagate up. This many, many times.

I have settled on the fact that I need a custom solution for the search. I
really do, but I can add sherding and read slaves as necessary! It won't be too
hard.

From the chessboard vector, how do I get a legal moves vector? And then how to
modify it. At that point it may become the fundamental chessboard implementation
in the engine, converted then to a inefficient board implementation. An
efficient and inefficient data structure, one to do the heavy lifting and the
other for the simple things. It could also be a perfect representation of the
chessboard vector. It must respect some conditions:

The strategic component of the chessboard must be highligthed. This doesn't mean
that after a single move the vector is almost untouched. This means that the
best moves are closer that the worse moves!
Basically, I want the distance between two chessboard vectors to be equal to the
strength of the move (or sequence of) between the two.

The strength of a move is a metric between 1 and 0, where 0 is a certainly
"worse-ing" move and 1 is a certainly "better-ing" move. The score of a game is
determined by my chance of winning, the opponent's chance of winning and the
uncertainty over these two values.

Una volta che trovo una rappresentazione embedded delle scacchiere con una parte
messa in risalto alla parte strategica, posso allenare una rete shallow per
ottenere una lista delle mosse migliori, con le mosse nonlegali a basso
punteggio o negative. Posso anche allenare una rete neurale per incrementare e
robe varie. La cosa importante e' che ho reinventato la visione degli scacchi.

The embedding must respect some conditions:
* The distance between two embeddings must be greater for similar chessboards,
  especially those easily reachable by a sequence of a grandmaster.
  Basically, the distance between two vectors equals (1 - strength) for each move
  needed to go from here to there.

Allenamento: ho come dataset un albero estremamento grande e profondo.

Devo trasformarlo in un albero di embedding e nel frattempo allenare delle reti
neurali.

A chess tensor is a tensor representing a chess position that satisfies certain
conditions:
* The distance between two chess tensors equals the likelihood that a perfect
  player will play that sequence of moves.

This means that there are infinitely many chess tensors even for a single chess
position, but some are more "true" than others. The dimensionality of chess
tensors is 1024. The distance is cosine similarity.
The distance to illegal positions.

I need to reinvent Z64C!
Basically, the classic search architecture works, but is limited by the huge
branching factor of chess and especially Go. So I need a search procedure that
actually searches inside a specific node and searches the perfect evaluation!
So:

I have a 1024-wide position tensor that is the evaluation of the current
position. This tensor is, however, not optimal. I need to seach the
1024-dimensions search space for ideas and tactics without actually making a
distinction between this or that position! Searching near the original point
basically simulates the search space, but instead of looking deeper and deeper
to improve an evaluation I can just look closer and closer and occasionally
deeper.

Un'altra cosa: a ogni chess tensor non associo una singola scacchiera, bensi'
piu' o meno una scacchiera. Ce ne sara' una piu' vicina alle altre, ma non e'
assolutamente ben distinto.


When Z64C needs to deepen a certain node, it creates and inserts a new node with
a score determined by its search algorithm, and then lowers the score of the
current node to the second best move (or 0 if all moves have been explored).
This assumes that the neural network gives 0 as output for illegal moves.

The questions that arise in this approach are:
* When do you stop searching?
  When I find one (or possibly more) line that has a satisfying score with good
  certainty. This part of the algorithm must take into serious consideration the
  uncertainty of the score, as it is key in the search: spend more time thinking
  in positions where your guts tell you that you might have a chance at getting
  a lead over your opponent. Search until the best lines either tell that you
  where wrong, or you were right. Act accordingly.
* How do you determine a newly created node's score?
  The score is a double-precision float and should be in the range 0-1, so that
  successive plies go to 0. It makes sense! A 0.95 move is almost forced and
  basically all lines follow that single move. The second best move could be
  like 0.03, at that point 0.03 will only be computed before 0.95 has a line
  that brings it down to 0.03. The alternation of colors doesn't alter the
  scores.
* How do you, after all of this, choose the best move?
  I need to formally prove this mathematically, but I have an intuition that can
  be used in a probablistic tree traversal algorithm.
* How do you remove all transpositions unreachable after the enemy's move in a
  short time?
  This also relates to the tree-like indexing of transpositions in the Redis
  server.
  I need to remove all positions (probably) unreacheable from the current position, so:
  This problem arises from the fact that I'm treating a tree (the search tree)
  with a hash table! I need to find an encoding.

Another question is: what do I set on nodes as an expiry date? White's turn
estimate plus black's turn estimate seems like a good choice. I'm gonna do it
like that.

In Redis, the sorted set has the numeric ID as keys.

The hash table has these keys:

* ID:tensor_in
* ID:tensor_out
* ID:board
* ID:parent
* ID:main
* ID:branch

TRAINING STRATEGY
=================
Training will use the modular architecture of the engine and train the various
neural networks with the outputs of the others.

I need to train:
* Hot2board
* Board2moves
* Increment2board

What's the relation between the estimated score [0-1] and the move strenght
[0-1]? Given one, one can find the other. So the move strenght is not relative
to other moves, but only to the estimated score.

I need an optimal algorithm.
Basically, it start playing moves given board positions. I add an exploration
component by playing also random or unlikely moves. At every move, I get an
estimated score and a move.

So my net is at the same time an actor and a critic: gives me an action (or more
than one), and an estimated winning score. How do I train this? I want to
bootstrap the winning score. I will train based on the advantage instead, so
that it's smarter about it.

Ok, maybe I got it. Using a normal optimizer like Adam, I backtrace part of the
net to maximize the estimated score, and I train the estimated score based on
the temporal difference learning formula. Now I should be set, I have the
training figured out, and I can devote my time to implement (and train!) Z64C
v0.2.

Notes on perft:
I want to develop a perft so that I don't need to worry about checks. It would
be much, much faster! What's the quirk?
A move is illegal if, at the next move, there is a pseudolegal move to capture the
king.

I need a struct Board where I can extremely rapidly say which moves are in the
tensor and if they are legal.

Cavalli: 8
Pedoni: 28
Torri: 14
Alfieri: 13
Re: 8

Architettura REDIS:
Sorted set con Id dei search node e priorita'.

Non capisco se dovrei fare l'update delle statistiche... mi serve davvero?
Spero davvero di no, perche' altrimenti sarei nei guai. Diventa un magnitudo di
complessita' temporale piu' lento. Mi serve che la ricerca sia velocissima. La
ricerca deve fare:

* Legality check (pseudo?)
  Come faccio a gestire una pseudo legality checking? Il problema non e' affatto
  lo scacco matto, ma lo stallo, che invece e' difficile da prevedere.

Ricerca:
Parto dal root node che non e' in Redis ma nello struct Engine e da li' aggiungo
a Redis i nodi di ricerca con la relativa priorita'. Una volta finito il tempo
di ricerca, tutti i nodi dell'albero vengono backtraced e si scopre il nodo
migliore.

La struttura in hash ha come key la scacchiera (ottimo), e come valore:
* Position tensor (scalable sia per one-hot che LSTM)
* Mossa della precendente
Il tutto in un memory dump di uno struct compatto by value.

La ricerca fa:
Prendo il nodo al top, prendo il position tensor, ci corro Tensorflow e ottengo
i nuovi nodi che metto dentro Redis.
La priorita' e' data da da una formula che dipende da TRAINING.

Se no training: allora la priorita' e' solo N, altrimenti ci sommo una
distrubuzione bounded normale

Il sorted set rappresenta i branch che devo esplorare con piu' urgenza.

Domanda: come influisce lo stallo sull'allenamento?

Qual e' l'algoritmo di ricerca?

0. Metti in focus il nodo di base.
1. Prendi il nodo di base e scegli una mossa.
non pseudolegali vengono subito eliminate e non inserite.
2. Se

Mi serve anche un algoritmo di backpropagation.

Ho una rete a parte, (7 strati) che mi calcola il passaggio da una scacchiera
ad un'altra. Le scacchiere memorizzano 64 dati per casella, quindi sono 4096 di
spessore.

Inoltre ho anche una rete a parte che mi traduce dal one-hot al tensor.

Quindi ho:
	16 per cavalli (2)
	48 per torri (3)
	48 per alfieri (3)
	32 per pedoni (8)
	 8 per re (1)

    152 move features
	3 state values
	Anche qua, derivo il learning rate da un fattore statistico.

	In input ho la scacchiera o il lstm piu':
	* Turn [2]
	* Castling rights [4]
	* En passant [8]
	Anziche' avere a che fare con i tempi, metto come output una descrizione
	piu' precisa dello state value cosi' so quali sono i punti piu' critici e
	quanto e' sicuro della propria azione.

Ho una architettura GRU con 140 uscite. Creo l'architettura in modo tale che
sia:

* Facilmente allenabile
* Pensi nel tempo e modifichi le propie previsioni (opzionale)
* Riporti la mossa automaticamente al GRU (sara' un GRU particolare, allora).

Question: how do I represent the estimated score? I don't want a single value,
instead I want a fully blown probability distribution. The distribution goes
from 0 to 1, where 0 and 1 are the fixed scores when the game is over. Anything
in between means it is not certain. How many values and what distribution to
choose? For example, I may want to draw a distribution where my enemy can't win,
I'm very close to winning but I may stalemate. How do I show that? I neet to
show uncertainty in an asymmetric way. I think I can describe the distribution
using 2 parameters:
* My score
* Uncertanty
The uncertainty drives the distribution towards the centre because there is
uncertainty.

The distribution is an S-shaped distribution in a [0-1] box both vertically and
horizontally.

Critic update algorithm: https://papers.nips.cc/paper/3290-temporal-difference-updating-without-a-learning-rate.pdf

Once I think about the probability distribution, I can use the extra information
to:
* Better drive game search: search more the more indecisive positions, don't
  search in clearly winning or losely positions. This favours quiet positions,
  just like real engines.
* Better train the critic.
* Can it also replace resoluteness factor? Yes. Instead of thinking about the
  most aggressive move, it outputs a simpler action space and I can select with
  the algorithm that I want.

I need to think about the learning process. I have almost everything figured
out.

Temporal difference learning:
Come modificatore uso:
* il numero della mossa
* material imbalance
Vedi
https://pdfs.semanticscholar.org/6e98/bbc9a3dc3ffb5e23abd748bf4e46d1671929.pdf
per migliorare LDleaf.

Procedura di training:
Crea una nuova scacchiera dall'inizio e gioca una partita. Questo per ogni
istanza, ce n'e' una per thread e sono asincrone (opzionale). Non puo' imparare
durante la partita per semplicita'. Durante la partita (giocata con un numero di
nodi fissi per partita) non eliminare le transpositions che non servono piu'.
values con anche i vari parametri e usi TD64 per calcorare gli state values
ottimali. Ci abbini le mosse migliori con una selezione bayesiana delle mosse
(non sempre la migliore) a metti tutto in fila al batch per l'allenamento.
Allenare il policy e' anche semplice (molto curiosamente), ma devo pensare al
TD64. TD64 e' la variante di TDlambda personalizzata per Z64C. TD(64) sara' una
funzione con una type signature particolare e che funziona molto bene.

Come funziona TD(64)?
Ho una lista di previsioni, un risultato della partita e devo trovare dove ci
sono stati errori... Ben cio'! Complicato. Anziche' distribuire equamente la
ricompensa, posso toglierla da una parte. E' plausibile che ci siano degli
errori in mezzo.

Faccio training sulle mosse di KingBase, e allo stesso tempo alleno con TD(64).
Mantengo il learning rate lo stesso e dopo comincio il self-play.


Devo anche usare CTDleaf con lambda 0.7


* Chess 960 (5%)
* Endgame tables
* Mixed opening book and not during training, so that it can learn to use it
  when useful and not to when it's not.
